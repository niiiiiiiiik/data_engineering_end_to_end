{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909a56bb-a533-44ea-9e75-dc3d828ab5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "data reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4164373-ad77-49ef-b140-b65d7364c33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "uncomment this when new columns are adding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3943a6fc-e6e8-4f30-8337-2a5174766caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6823145c-905a-40fd-850b-854eacad0abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configure your ADLS paths\n",
    "bronze_source_path     = \"abfss://source@cajainterview.dfs.core.windows.net/discharge\"\n",
    "bronze_checkpoint_dir  = \"abfss://bronze@cajainterview.dfs.core.windows.net/checkpoint_dailydischarge\"\n",
    "bronze_schema_dir      = f\"{bronze_checkpoint_dir}/schemas\"\n",
    "bronze_sink_path       = \"abfss://bronze@cajainterview.dfs.core.windows.net/dailydischarge\"\n",
    "bronze_bad_path        = \"abfss://bronze@cajainterview.dfs.core.windows.net/badrecords\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13792f7-2e1d-49d4-94f7-c92b5e200ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Stop any prior streams and clear old checkpoints/schemas\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "dbutils.fs.rm(bronze_checkpoint_dir, recurse=True)\n",
    "dbutils.fs.rm(bronze_schema_dir, recurse=True)\n",
    "dbutils.fs.rm(bronze_bad_path, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce80a760-c076-48dc-b362-3cf0ca51ee33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Stream-read with Auto Loader\n",
    "bronze_df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")  # <— ALIGN BY HEADER\n",
    "        .option(\"inferSchema\", \"true\")  # <— INFER types once\n",
    "        .option(\"cloudFiles.schemaLocation\", bronze_schema_dir)  # store AVRO schemas\n",
    "        .option(\"cloudFiles.maxFilesPerTrigger\", \"1\")  # control incremental pace\n",
    "        .option(\"badRecordsPath\", bronze_bad_path)  # route malformed rows\n",
    "        .load(bronze_source_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0756861-2c8d-4146-9ea0-c0cd15d8f39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# after your readStream with addNewColumns, but before writeStream\n",
    "bronze_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42680c2-549e-4345-9874-c4a4009e88e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "data writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c5b118-aa5c-43f9-b29e-29b0f29e4e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Write to Bronze Delta with schema merging off  \n",
    "#    (we’re inferring schema, so no need to merge later)\n",
    "(\n",
    "    bronze_df.writeStream\n",
    "        .format(\"parquet\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{bronze_checkpoint_dir}/checkpoint\")\n",
    "        .trigger(once=True)\n",
    "        .start(bronze_sink_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdab4f7-7f0d-4182-8c3e-4eb7f123d3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet=spark.read.format(\"parquet\")\\\n",
    "    .load(\"abfss://bronze@cajainterview.dfs.core.windows.net/dailydischarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b88b93-1c97-4cbc-9fb4-984efc0ba4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff76ad31-ba0f-4250-8121-703ce2adeb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# assume your DataFrame is called df\n",
    "# 1. Parse the Period column into a proper DateType\n",
    "df2 = df_parquet.withColumn(\n",
    "    \"Period_date\",\n",
    "    F.to_date(F.col(\"Period\"), \"dd/MM/yyyy\")\n",
    ")\n",
    "\n",
    "# 2. Aggregate to get min and max\n",
    "min_max = df2.agg(\n",
    "    F.min(\"Period_date\").alias(\"min_period\"),\n",
    "    F.max(\"Period_date\").alias(\"max_period\")\n",
    ")\n",
    "\n",
    "min_max.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390da8a9-610e-4941-b9e3-8240e8d40adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df_parquet.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_parquet.columns])\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd0379a-6f89-4f57-9718-f479c924ea81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_periods = df_parquet.groupBy(\"Period\").count()\n",
    "display(unique_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15561109-de35-422c-a68a-f553ed82c204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_r = df_parquet.groupBy(\"_rescued_data\").count()\n",
    "display(unique_r)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}